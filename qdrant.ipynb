{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f772b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "client = QdrantClient(path=\"qdrant_data\")\n",
    "\n",
    "# client = QdrantClient(\n",
    "#     url=\"https://b5e3e25c-0644-477b-8c25-76b8e3c4fb7a.us-east-1-0.aws.cloud.qdrant.io:6333\", \n",
    "#     api_key=qdrant_api_key,\n",
    "# )\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"demo_collection\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b23c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "159da827",
   "metadata": {},
   "source": [
    "### Save embedding to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c87846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "def load_split(file_path):\n",
    "    # Load Markdown file\n",
    "    loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    # loader = UnstructuredMarkdownLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Extract raw text from the first Document object\n",
    "    markdown_text = documents[0].page_content  # âœ… Use page_content string\n",
    "\n",
    "    # Markdown splitter by headers (e.g., \"# Page\")\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"##\", \"page\")])\n",
    "    docs = markdown_splitter.split_text(markdown_text)  # âœ… Now it's a string\n",
    "\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef45aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./knowledge/combined.md\"\n",
    "chunks = load_split(file_path)\n",
    "vector_store.add_documents(documents=chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_generator(vectordb, query, llm):\n",
    "    context = vectordb.similarity_search(query, k=4)\n",
    "\n",
    "    # for res in results:\n",
    "    #     print(f\"ðŸ“„ {res.page_content} [{res.metadata}]\")\n",
    "    template = f\"\"\"\n",
    "    You are an intelligent assistant designed to provide accurate and concise answers based on the context provided. \n",
    "    Follow these rules strictly:\n",
    "    1. Use ONLY the information provided in the context to answer the question.\n",
    "    2. If the context does not contain enough information to answer the question, say \"I don't know.\"\n",
    "    3. Do not make up or assume any information outside of the context.\n",
    "    4. Keep your answer concise and to the point (maximum 3 sentences).\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Helpful Answer:\n",
    "    \"\"\"\n",
    "    ans = llm.invoke(template)\n",
    "    return ans.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f554c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def response_generatorr(vectordb, query, llm):\n",
    "    # vectordb = get_vectordb()\n",
    "    # llm = get_llm()\n",
    "    context = vectordb.similarity_search(query, k=4)\n",
    "\n",
    "    # for res in results:\n",
    "    #     print(f\"ðŸ“„ {res.page_content} [{res.metadata}]\")\n",
    "    template = f\"\"\"\n",
    "    You are an intelligent assistant designed to provide accurate and concise answers based on the context provided. \n",
    "    Follow these rules strictly:\n",
    "    1. Use ONLY the information provided in the context to answer the question.\n",
    "    2. If the context does not contain enough information to answer the question, say \"I don't know.\"\n",
    "    3. Do not make up or assume any information outside of the context.\n",
    "    4. Keep your answer concise and to the point (maximum 3 sentences).\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Helpful Answer:\n",
    "    \"\"\"\n",
    "    ans = llm.invoke(template)\n",
    "    return ans.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0a13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
