{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG\n",
    "\n",
    "**Tech Stack** \n",
    "1. vectordatabase - ChramaDB\n",
    "2. sentence embedding - all-MiniLM-L6-v2\n",
    "3. llm - llama3-8b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "# llm = ChatOllama(\n",
    "#     model = \"deepseek-r1:1.5b\",\n",
    "#     temperature = 0,\n",
    "#     num_predict = 256,\n",
    "#     # other params ...\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "def embd_load_vectordb(filepath):\n",
    "    # Initialize the embedding model\n",
    "    embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        # Load and split the PDF document\n",
    "    docs = PyPDFLoader(filepath).load_and_split()\n",
    "    # Create a Chroma vector store with a specified directory for persistence\n",
    "    vectordb = Chroma.from_documents(docs, embedding, persist_directory=\"./test_db\")\n",
    "    print(\"Vector database created and persisted.\")\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "def load_vectordb():\n",
    "    embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    loaded_db = Chroma(persist_directory=\"./test_db\", embedding_function=embedding)\n",
    "    return loaded_db\n",
    "    \n",
    "vectordb = load_vectordb()\n",
    "\n",
    "\n",
    "\n",
    "def response_generator(vectordb, query, llm):\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. {context} Question: {question} Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, \n",
    "                                           retriever=vectordb.as_retriever(), \n",
    "                                           return_source_documents=True, \n",
    "                                           chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "\n",
    "    ans = qa_chain.invoke(query)\n",
    "    return ans[\"result\"]\n",
    "\n",
    "\n",
    "query = \"what are the side effects of Ondansetron\"\n",
    "ans = response_generator(vectordb, query, llm)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "# llm = ChatOllama(\n",
    "#     model = \"deepseek-r1:1.5b\",\n",
    "#     temperature = 0,\n",
    "#     num_predict = 256,\n",
    "#     # other params ...\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embd_load_vectordb(filepath):\n",
    "    # Initialize the embedding model\n",
    "    embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        # Load and split the PDF document\n",
    "    docs = PyPDFLoader(filepath).load_and_split()\n",
    "    # Create a Chroma vector store with a specified directory for persistence\n",
    "    vectordb = Chroma.from_documents(docs, embedding, persist_directory=\"./test_db\")\n",
    "    print(\"Vector database created and persisted.\")\n",
    "    return vectordb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectordb():\n",
    "    embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    loaded_db = Chroma(persist_directory=\"./test_db\", embedding_function=embedding)\n",
    "    return loaded_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_generator(vectordb, query, llm):\n",
    "    template = \"\"\"Use the following pieces of context to answer the given question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Instructions:\n",
    "    1. Base your answer only on the provided context\n",
    "    2. If the context doesn't contain relevant information, say \"I don't have enough information to answer this question.\"\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "\n",
    "\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, \n",
    "                                           retriever=vectordb.as_retriever(), \n",
    "                                           return_source_documents=True, \n",
    "                                           chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "\n",
    "    ans = qa_chain.invoke(query)\n",
    "    return ans[\"result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emed_data = embd_load_vectordb(\"./knowledge/health_products_data.pdf\")\n",
    "vectordb = load_vectordb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the side effects of Ondansetron\"\n",
    "answer = response_generator(vectordb, query, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG (Dense Passage Retrieval (DPR) Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load DPR Model\n",
    "query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base')\n",
    "passage_encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "documents = PyPDFLoader(\"./knowledge/health_products_data.pdf\").load_and_split()\n",
    "passages = [doc.page_content for doc in documents]  # Extract text from Document objects\n",
    "\n",
    "# Encode Passages into Dense Vectors\n",
    "passage_embeddings = passage_encoder.encode(passages, convert_to_numpy=True)\n",
    "\n",
    "# Create FAISS Index\n",
    "dimension = passage_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(passage_embeddings)\n",
    "\n",
    "# Encode Query\n",
    "query = \"which medicine could be usefull for knee pain\"\n",
    "query_embedding = query_encoder.encode([query], convert_to_numpy=True)\n",
    "\n",
    "# Perform Similarity Search\n",
    "k = 2  # Retrieve top-2 passages\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Print Results\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop Relevant Passages:\")\n",
    "for i in range(k):\n",
    "    print(f\"{i+1}. {passages[indices[0][i]]} (Distance: {distances[0][i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name=\"Llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DPR Model\n",
    "query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base')\n",
    "passage_encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = PyPDFLoader(\"./knowledge/health_products_data.pdf\").load_and_split()\n",
    "passages = [doc.page_content for doc in documents]  # Extract text from Document objects\n",
    "\n",
    "# Encode Passages into Dense Vectors\n",
    "passage_embeddings = passage_encoder.encode(passages, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create FAISS Index\n",
    "dimension = passage_embeddings.shape[1]\n",
    "vectordb = faiss.IndexFlatL2(dimension)\n",
    "vectordb.add(passage_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_generator(passages, query, llm):\n",
    "    query_embedding = query_encoder.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    # Perform Similarity Search\n",
    "    k = 2  # Retrieve top-2 passages\n",
    "    distances, indices = vectordb.search(query_embedding, k)\n",
    "\n",
    "    # Extract relevant passages\n",
    "    context = [passages[i] for i in indices[0].tolist()]  # Convert NumPy array to list\n",
    "\n",
    "    # Construct prompt\n",
    "    template = f\"\"\"\n",
    "    You are an intelligent assistant designed to provide accurate and concise answers based on the context provided. \n",
    "    Follow these rules strictly:\n",
    "    1. Use ONLY the information provided in the context to answer the question.\n",
    "    2. If the context does not contain enough information to answer the question, say \"I don't know.\"\n",
    "    3. Do not make up or assume any information outside of the context.\n",
    "    4. Keep your answer concise and to the point (maximum 3 sentences).\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Helpful Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate response using LLM\n",
    "    res = llm.invoke(template)\n",
    "    return res.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wellbutrin (bupropion), Remeron (mirtazapine), and Trazodone are antidepressant medications used to treat depression.\n"
     ]
    }
   ],
   "source": [
    "query = \"which medicine is used to treat depression\"\n",
    "ans = response_generator(passages, query, llm)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "load_dotenv()\n",
    "import numpy as np\n",
    "\n",
    "llm = ChatGroq(model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "# llm = ChatOllama(\n",
    "#     model = \"deepseek-r1:1.5b\",\n",
    "#     temperature = 0,\n",
    "#     num_predict = 256,\n",
    "#     # other params ...\n",
    "# )\n",
    "\n",
    "# Load DPR Model\n",
    "query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base')\n",
    "passage_encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "documents = PyPDFLoader(\"./knowledge/health_products_data.pdf\").load_and_split()\n",
    "passages = [doc.page_content for doc in documents]  # Extract text from Document objects\n",
    "\n",
    "\n",
    "# Encode Passages into Dense Vectors\n",
    "passage_embeddings = passage_encoder.encode(passages, convert_to_numpy=True)\n",
    "\n",
    "# Create FAISS Index\n",
    "dimension = passage_embeddings.shape[1]\n",
    "vectordb = faiss.IndexFlatL2(dimension)\n",
    "vectordb.add(passage_embeddings)\n",
    "\n",
    "def response_generator(passages, query, llm):\n",
    "\n",
    "    query_embedding = query_encoder.encode([query], convert_to_numpy=True)\n",
    "    # Perform Similarity Search\n",
    "    k = 2  # Retrieve top-2 passages\n",
    "    indices = vectordb.search(query_embedding, k)\n",
    "    context = [passages[i] for i in indices[0]] \n",
    "\n",
    "    template = f\"\"\"\n",
    "    You are an intelligent assistant designed to provide accurate and concise answers based on the context provided. \n",
    "    Follow these rules strictly:\n",
    "    1. Use ONLY the information provided in the context to answer the question.\n",
    "    2. If the context does not contain enough information to answer the question, say \"I don't know.\"\n",
    "    3. Do not make up or assume any information outside of the context.\n",
    "    4. Keep your answer concise and to the point (maximum 3 sentences).\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Helpful Answer:\n",
    "    \"\"\"\n",
    "    res = llm.invoke(template)\n",
    "    return res.content\n",
    "\n",
    "\n",
    "query = \"which medicine is used to treat depression\"\n",
    "ans= response_generator(passages, query, llm)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "load_dotenv()\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "# llm = ChatOllama(\n",
    "#     model = \"deepseek-r1:1.5b\",\n",
    "#     temperature = 0,\n",
    "#     num_predict = 256,\n",
    "#     # other params ...\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DPR Model\n",
    "query_encoder = SentenceTransformer('facebook-dpr-question_encoder-single-nq-base')\n",
    "passage_encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = PyPDFLoader(\"./knowledge/health_products_data.pdf\").load_and_split()\n",
    "passages = [doc.page_content for doc in documents]  # Extract text from Document objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Passages into Dense Vectors\n",
    "passage_embeddings = passage_encoder.encode(passages, convert_to_numpy=True)\n",
    "\n",
    "# Create FAISS Index\n",
    "dimension = passage_embeddings.shape[1]\n",
    "vectordb = faiss.IndexFlatL2(dimension)\n",
    "vectordb.add(passage_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_generator(passages, query, llm):\n",
    "\n",
    "    query_embedding = query_encoder.encode([query], convert_to_numpy=True)\n",
    "    # Perform Similarity Search\n",
    "    k = 2  # Retrieve top-2 passages\n",
    "    indices = vectordb.search(query_embedding, k)\n",
    "    context = [passages[i] for i in indices[0]] \n",
    "\n",
    "    template = f\"\"\"\n",
    "    You are an intelligent assistant designed to provide accurate and concise answers based on the context provided. \n",
    "    Follow these rules strictly:\n",
    "    1. Use ONLY the information provided in the context to answer the question.\n",
    "    2. If the context does not contain enough information to answer the question, say \"I don't know.\"\n",
    "    3. Do not make up or assume any information outside of the context.\n",
    "    4. Keep your answer concise and to the point (maximum 3 sentences).\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Helpful Answer:\n",
    "    \"\"\"\n",
    "    res = llm.invoke(template)\n",
    "    return res.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"which medicine is used to treat depression\"\n",
    "ans= response_generator(passages, query, llm)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
