{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_groq import ChatGroq\n",
    "import tempfile\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(file_path):\n",
    "    all_splits = []\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    data = loader.load_and_split()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    splits = text_splitter.split_documents(data)\n",
    "    all_splits.extend(splits)\n",
    "    \n",
    "    embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(all_splits, embedding)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = load_and_process_data(\"ML.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def response_generator(vectordb, query, llm):\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. {context} Question: {question} Helpful Answer:\"\"\"\n",
    "    \n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"], \n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm, \n",
    "        retriever=vectordb.as_retriever(), \n",
    "        return_source_documents=True, \n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()  # Start the timer\n",
    "    result = qa_chain({\"query\": query})\n",
    "    end_time = time.time()  # End the timer\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate time taken\n",
    "    # print(f\"Response Time: {elapsed_time:.2f} seconds\")  # Print the time\n",
    "    \n",
    "    return result[\"result\"], elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name=\"Llama3-8b-8192\")\n",
    "query = \"what is machine learning\"\n",
    "a = response_generator(vectordb, query,llm)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name=\"Llama3-8b-8192\")\n",
    "llm = OllamaLLM(model=\"mistral\")\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_queries(vectordb, queries, llm):\n",
    "    \"\"\"\n",
    "    Processes multiple queries, measuring the response time for each.\n",
    "    Returns a pandas DataFrame with query, result, and time taken.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for query in queries:\n",
    "        result, time_taken = response_generator(vectordb, query, llm)\n",
    "        results.append({\n",
    "            \"Query\": query,\n",
    "            \"Result\": result,\n",
    "            \"Time Taken (seconds)\": time_taken\n",
    "        })\n",
    "\n",
    "    # Convert results into a pandas DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `vectordb` and `llm` are defined\n",
    "queries = [\n",
    "    \"what is Machine Learning\",\n",
    "    \"who is the Prime minister of India\",\n",
    "    \"what is ensemble learning technique\",\n",
    "    \"what is PCA\",\n",
    "    \"who is the founder of Tesla\"\n",
    "]\n",
    "\n",
    "llm = ChatGroq(model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "# Process the queries and generate the dataset\n",
    "groq_res = process_queries(vectordb, queries, llm)\n",
    "\n",
    "# Print the dataset\n",
    "print(groq_res)\n",
    "\n",
    "# Save the dataset as a CSV file\n",
    "\n",
    "groq_res.to_csv(\"llm_comparison_results/groq_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `vectordb` and `llm` are defined\n",
    "queries = [\n",
    "    \"what is Machine Learning\",\n",
    "    \"who is the Prime minister of India\",\n",
    "    \"what is ensemble learning technique\",\n",
    "    \"what is PCA\",\n",
    "    \"who is the founder of Tesla\"\n",
    "]\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "# Process the queries and generate the dataset\n",
    "ollama_lamma3_2_res = process_queries(vectordb, queries, llm)\n",
    "\n",
    "# Print the dataset\n",
    "print(ollama_lamma3_2_res)\n",
    "\n",
    "# Save the dataset as a CSV file\n",
    "\n",
    "ollama_lamma3_2_res.to_csv(\"llm_comparison_results/ollama_lamma3_2_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `vectordb` and `llm` are defined\n",
    "queries = [\n",
    "    \"what is Machine Learning\",\n",
    "    \"who is the Prime minister of India\",\n",
    "    \"what is ensemble learning technique\",\n",
    "    \"what is PCA\",\n",
    "    \"who is the founder of Tesla\"\n",
    "]\n",
    "\n",
    "llm = OllamaLLM(model=\"mistral\")\n",
    "\n",
    "# Process the queries and generate the dataset\n",
    "ollama_mistral_res = process_queries(vectordb, queries, llm)\n",
    "\n",
    "# Print the dataset\n",
    "print(ollama_mistral_res)\n",
    "\n",
    "# Save the dataset as a CSV file\n",
    "\n",
    "ollama_mistral_res.to_csv(\"llm_comparison_results/ollama_mistral_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"llm_comparison_results/groq_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # or \"PleIAs/Pleias-nano-1B-RAG\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model,tokenizer,input_query):\n",
    "    # Example input query\n",
    "    start_time = time.time()  # Start the timer\n",
    "    input_query = \"What are the benefits of using RAG in language models?\"\n",
    "    inputs = tokenizer(input_query, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "    # Generate a response\n",
    "    output_sequences = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=150,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    end_time = time.time()  # End the timer\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    return response, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(f\"tokenizer/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.save_pretrained(f\"model/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"tokenizer/{model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"model/{model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to generate poem on the text given below: happy man\n",
      "The happy man, who's always smiling,\n",
      "With a heart full of love and care.\n",
      "He's not just a face in the crowd,\n",
      "But a beacon of hope for all.\n",
      "\n",
      "In his hands, he holds so much,\n",
      "From a cup of tea to a meal.\n",
      "A smile that brightens\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"\"\"Your task is to generate poem on the text given below: happy man\"\"\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens= 60)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Specify your model name\n",
    "model_name=\"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"tokenizer/{model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"model/{model_name}\")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to generate poem on the text given below: happy man\n",
      "\n",
      "The text provided is \"happy man\". You are required to write a poem about this word. The poem should be written in English and should not contain any other words apart from those mentioned in the text.\n",
      "\n",
      "Please provide me with a poem that describes the concept of a \"happy man\" in your\n"
     ]
    }
   ],
   "source": [
    "# Prepare input text\n",
    "input_text = f\"\"\"Your task is to generate poem on the text given below: happy man\"\"\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # Move input tensor to GPU\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**input_ids, max_new_tokens=60)\n",
    "\n",
    "# Decode and print the output\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is machine learning? How does it work?\n",
      "\n",
      "Machine Learning is a subfield of artificial intelligence that involves the development and application of algorithms to enable computers to learn from data without being explicitly programmed. It allows machines to automatically improve their performance based on new information or experience, rather than being manually programmed.\n",
      "\n",
      "The basic idea behind machine\n"
     ]
    }
   ],
   "source": [
    "def generate_output(input_text):\n",
    "   # Move model to GPU\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    # Prepare input text\n",
    "    input_text = \"What is machine learning?\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # Move input tensor to GPU\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=60)\n",
    "\n",
    "    # Decode and print the output\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
