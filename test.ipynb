{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load and split the PDF document\n",
    "docs = PyPDFLoader(\"doc.pdf\").load_and_split()\n",
    "\n",
    "# Create a Chroma vector store with a specified directory for persistence\n",
    "db = Chroma.from_documents(docs, embedding, persist_directory=\"./chroma_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load the previously saved Chroma vector store\n",
    "loaded_db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding)\n",
    "\n",
    "# Now you can perform retrieval operations using loaded_db\n",
    "query = \"What is this document about?\"\n",
    "results = loaded_db.similarity_search(query,k=5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace LLM for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'? It\\'s a way of teaching computers to help us make smart decisions.\"\\n\\n\"It\\'s not as difficult as you think,\" said, \"it\\'s just a lot of hard work.\"\\n\\n\"There\\'s a lot of work to do,\" said Andy, \"but it\\'s worth it. And it\\'s good for you. I hope it helps you learn to think.\"'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "repo_id=\"mayura25/SmolLM2-135M-medical-dataset-finetuned-merged\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=128,temperature=0.7)\n",
    "llm.invoke(\"what is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seeds, one on pumpkin seeds, and one on pumpkin seeds.\n",
      "\n",
      "### Step 1: Write a One-Word Summary\n",
      "Answer the question truthfully, write a summary that appropriately completes the request.\n",
      "\n",
      "### Step 2: Read the Story Thoroughly\n",
      "Plug the query into your browser, and read the entire story. Read the story aloud, and try to write the summary as well.\n",
      "\n",
      "### Step 3: Write a One-Word Response\n",
      "Answer the question truthfully, write a response that appropriately completes the request.\n"
     ]
    }
   ],
   "source": [
    "a = llm.invoke(\"Create one story on pumpkin\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"PleIAs/Pleias-pico-350m-RAG\"  # or \"PleIAs/Pleias-nano-1B-RAG\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mayur\\.cache\\huggingface\\hub\\models--PleIAs--Pleias-pico-350m-RAG. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"PleIAs/Pleias-pico-350m-RAG\"  # or \"PleIAs/Pleias-nano-1B-RAG\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Example input query\n",
    "input_query = \"What are the benefits of using RAG in language models?\"\n",
    "inputs = tokenizer(input_query, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Generate a response\n",
    "output_sequences = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=150,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "response = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input query\n",
    "input_query = \"What are the benefits of using RAG in language models?\"\n",
    "inputs = tokenizer(input_query, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the benefits of using RAG in language models?\n",
      "\n",
      "\n",
      "In this article, we present a large-scale crosslinguistic evaluation of the accuracy and model interpretability of RAG. We test a large dataset of 9 million parallel corpora of the largest open-domain sentence pairs available: over 1,500 of the largest corpora from the Penn Treebank corpus. For the sake of the evaluation, we develop a novel RAG corpus for English and Chinese, as well as two other languages for each. Our evaluation is based on the task of understanding the meaning of sentences. We consider two different evaluation benchmarks, including the human test set and the publicly available test set of the Universal Dependencies shared task, and report\n"
     ]
    }
   ],
   "source": [
    "# Generate a response\n",
    "output_sequences = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=150,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "response = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./local_model\\\\tokenizer_config.json',\n",
       " './local_model\\\\special_tokens_map.json',\n",
       " './local_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the model name\n",
    "model_name = \"PleIAs/Pleias-pico-350m-RAG\"  # Change this to your desired model\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Save the model and tokenizer locally\n",
    "model.save_pretrained(\"./local_model\")\n",
    "tokenizer.save_pretrained(\"./local_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the benefits of using RAG in language models? (previously: Language Model Stack Exchange). In this section, we will review some of the benefits of using a language model, such as transfer learning, a subset of fine-tuning, pre-training, and fine-tuning.\n",
      "• Transfer learning: Language models are good at predicting new words from new words, so it's easier to transfer knowledge learned during training on text.\n",
      "• Use cases:\n",
      "    • Fewer text data: Using a language model on fewer resources can help prevent overfitting.\n",
      "• Fine-tuning: Fine-tuning a model from scratch can help improve performance on a large dataset by fine-tuning a model in a\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer from local storage\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./local_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./local_model\")\n",
    "# Example input query\n",
    "input_query = \"What are the benefits of using RAG in language models?\"\n",
    "inputs = tokenizer(input_query, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "output_sequences = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=150,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "response = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is MLB 2.0?\n",
      "\n",
      "3\n",
      "\n",
      "MLB 2.0 is an English language online poker game created by MLB 2.0 in 2014. The game has features such as bonus rounds, a feature called “Winning Rules”, allowing players to collect bonus poker chips from all opponents of a draw. You can also purchase bonus chips from the online game to your advantage.\n",
      "The games were released in 2014 at the end of the season. During the season, you can also get access to bonus points. At the end of the season, all winning players in the games will receive a “winners bonus”. The game continues to operate for the rest of the year.\n",
      "The current official website of ML\n"
     ]
    }
   ],
   "source": [
    "input_query = \"What is ML\"\n",
    "inputs = tokenizer(input_query, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "output_sequences = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=150,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "response = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and setting up PleIAs/pleias_350m_rag...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m CACHE_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Local directory to store models\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Download and set up\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_and_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCACHE_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[0;32m     79\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe best thing about artificial intelligence is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m, in \u001b[0;36mdownload_and_setup_model\u001b[1;34m(model_name, cache_dir)\u001b[0m\n\u001b[0;32m     23\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     24\u001b[0m     model_name,\n\u001b[0;32m     25\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m     26\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Download and set up model\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use float16 for better memory efficiency\u001b[39;49;00m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Automatically handle device placement\u001b[39;49;00m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:3589\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3585\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3586\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3587\u001b[0m         )\n\u001b[0;32m   3588\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 3589\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   3590\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3591\u001b[0m         )\n\u001b[0;32m   3593\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[0;32m   3594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[1;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "def download_and_setup_model(model_name, cache_dir=None):\n",
    "    \"\"\"\n",
    "    Downloads and sets up a model and tokenizer from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model on Hugging Face (e.g., 'facebook/opt-350m')\n",
    "        cache_dir (str, optional): Directory to store the downloaded model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    print(f\"Downloading and setting up {model_name}...\")\n",
    "    \n",
    "    # Set up cache directory if provided\n",
    "    if cache_dir:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Download and set up tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_dir,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Download and set up model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_dir,\n",
    "        torch_dtype=torch.float16,  # Use float16 for better memory efficiency\n",
    "        device_map=\"auto\",  # Automatically handle device placement\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(prompt, model, tokenizer, max_length=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate text using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Input text to generate from\n",
    "        model: Loaded language model\n",
    "        tokenizer: Loaded tokenizer\n",
    "        max_length (int): Maximum length of generated text\n",
    "        temperature (float): Controls randomness (higher = more random)\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and return text\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example with a small model\n",
    "    MODEL_NAME = \"PleIAs/pleias_350m_rag\"  # You can change this to any model on HuggingFace\n",
    "    CACHE_DIR = \"models\"  # Local directory to store models\n",
    "    \n",
    "    # Download and set up\n",
    "    model, tokenizer = download_and_setup_model(MODEL_NAME, CACHE_DIR)\n",
    "    \n",
    "    # Generate text\n",
    "    prompt = \"The best thing about artificial intelligence is\"\n",
    "    generated_text = generate_text(prompt, model, tokenizer)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrewAI Agentic workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model_name=\"Llama3-8b-8192\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = llm.invoke(\"what is machine learning\")\n",
    "# print(a.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chatbot_response(inpt):\n",
    "    llm = ChatGroq(model_name=\"Llama3-8b-8192\")\n",
    "    res = llm.invoke(inpt)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"chain of thought\": \n",
      "The user's input seems to be related to making a purchase, specifically ordering a new coffee. This input is not asking for general information about the coffee shop, nor is it asking for a recommendation. It's a clear indication that the user wants to place an order.\n",
      "\n",
      "\"decision\": order_taking_agent\n",
      "\n",
      "\"message\":\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "\n",
    "def get_chatbot_response(inpt):\n",
    "    llm = ChatGroq(model_name=\"Llama3-8b-8192\")\n",
    "    res = llm.invoke(inpt)\n",
    "    return res.content\n",
    "\n",
    "def get_response(query):\n",
    "        # messages = deepcopy(messages)\n",
    "\n",
    "        system_prompt = f\"\"\"\n",
    "            You are a helpful AI assistant for a coffee shop application.\n",
    "            Your task is to determine what agent should handle the user input. You have 3 agents to choose from:\n",
    "            1. details_agent: This agent is responsible for answering questions about the coffee shop, like location, delivery places, working hours, details about menue items. Or listing items in the menu items. Or by asking what we have.\n",
    "            2. order_taking_agent: This agent is responsible for taking orders from the user. It's responsible to have a conversation with the user about the order untill it's complete.\n",
    "            3. recommendation_agent: This agent is responsible for giving recommendations to the user about what to buy. If the user asks for a recommendation, this agent should be used.\n",
    "\n",
    "            Your output should be in a structured json format like so. each key is a string and each value is a string. Make sure to follow the format exactly:\n",
    "            \"chain of thought\": go over each of the agents above and write some your thoughts about what agent is this input relevant to.\n",
    "            \"decision\": \"details_agent\" or \"order_taking_agent\" or \"recommendation_agent\". Pick one of those. and only write the word.\n",
    "            \"message\": leave the message empty.\n",
    "            \n",
    "\n",
    "            user Input :\n",
    "            {query}\n",
    "            \"\"\"\n",
    "        \n",
    "        return get_chatbot_response(system_prompt)\n",
    "        \n",
    "    \n",
    "a = get_response(\"I wanna order new coffee\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "\n",
    "class ClassificationAgent():\n",
    "    # def __init__(self):\n",
    "    \n",
    "    def get_response(self,messages):\n",
    "        # messages = deepcopy(messages)\n",
    "\n",
    "        system_prompt = \"\"\"\n",
    "            You are a helpful AI assistant for a coffee shop application.\n",
    "            Your task is to determine what agent should handle the user input. You have 3 agents to choose from:\n",
    "            1. details_agent: This agent is responsible for answering questions about the coffee shop, like location, delivery places, working hours, details about menue items. Or listing items in the menu items. Or by asking what we have.\n",
    "            2. order_taking_agent: This agent is responsible for taking orders from the user. It's responsible to have a conversation with the user about the order untill it's complete.\n",
    "            3. recommendation_agent: This agent is responsible for giving recommendations to the user about what to buy. If the user asks for a recommendation, this agent should be used.\n",
    "\n",
    "            Your output should be in a structured json format like so. each key is a string and each value is a string. Make sure to follow the format exactly:\n",
    "            {\n",
    "            \"chain of thought\": go over each of the agents above and write some your thoughts about what agent is this input relevant to.\n",
    "            \"decision\": \"details_agent\" or \"order_taking_agent\" or \"recommendation_agent\". Pick one of those. and only write the word.\n",
    "            \"message\": leave the message empty.\n",
    "            }\n",
    "            \"\"\"\n",
    "        \n",
    "        input_messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "        ]\n",
    "\n",
    "        input_messages += messages[-3:]\n",
    "\n",
    "        chatbot_output =get_chatbot_response(self.client,self.model_name,input_messages)\n",
    "        output = self.postprocess(chatbot_output)\n",
    "        return output\n",
    "\n",
    "    def postprocess(self,output):\n",
    "        output = json.loads(output)\n",
    "\n",
    "        dict_output = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": output['message'],\n",
    "            \"memory\": {\"agent\":\"classification_agent\",\n",
    "                       \"classification_decision\": output['decision']\n",
    "                      }\n",
    "        }\n",
    "        return dict_output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pinecone'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent, Task, Crew, Process\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pinecone\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pinecone'"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from typing import List, Dict\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "class HealthcareEcommerceCrew:\n",
    "    def __init__(self, openai_api_key: str, pinecone_api_key: str, pinecone_environment: str):\n",
    "        # Initialize API keys and environment\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "        pinecone.init(api_key=pinecone_api_key, environment=pinecone_environment)\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.vector_store = Pinecone.from_existing_index(\n",
    "            index_name=\"healthcare-products\",\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "\n",
    "    def create_agents(self):\n",
    "        # Security Agent\n",
    "        self.security_agent = Agent(\n",
    "            role='Security Agent',\n",
    "            goal='Ensure all interactions are safe and comply with healthcare regulations',\n",
    "            backstory=\"\"\"You are an AI security expert specialized in healthcare compliance \n",
    "            and data protection. You verify all interactions for safety and regulatory compliance.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=False,\n",
    "            tools=[self.verify_safety]\n",
    "        )\n",
    "\n",
    "        # Input Classifier Agent\n",
    "        self.classifier_agent = Agent(\n",
    "            role='Input Classifier',\n",
    "            goal='Accurately classify and route user inputs to appropriate agents',\n",
    "            backstory=\"\"\"You are an expert in natural language processing and intent classification. \n",
    "            You analyze user queries to determine their primary intent and route them accordingly.\"\"\",\n",
    "            verbose=True,\n",
    "            allow_delegation=True\n",
    "        )\n",
    "\n",
    "        # Order Agent\n",
    "        self.order_agent = Agent(\n",
    "            role='Order Manager',\n",
    "            goal='Process and manage product orders efficiently',\n",
    "            backstory=\"\"\"You are an e-commerce specialist handling all aspects of order processing. \n",
    "            You ensure orders are processed correctly and maintain order status information.\"\"\",\n",
    "            verbose=True,\n",
    "            tools=[self.process_order, self.check_order_status]\n",
    "        )\n",
    "\n",
    "        # Cart Agent\n",
    "        self.cart_agent = Agent(\n",
    "            role='Cart Manager',\n",
    "            goal='Manage shopping cart operations and display cart contents',\n",
    "            backstory=\"\"\"You handle all shopping cart related operations, including adding/removing items \n",
    "            and displaying current cart contents.\"\"\",\n",
    "            verbose=True,\n",
    "            tools=[self.update_cart, self.display_cart]\n",
    "        )\n",
    "\n",
    "        # Comparison Agent\n",
    "        self.comparison_agent = Agent(\n",
    "            role='Product Comparator',\n",
    "            goal='Compare products with market alternatives',\n",
    "            backstory=\"\"\"You analyze and compare products with similar offerings in the market. \n",
    "            You provide detailed comparisons of features, benefits, and pricing.\"\"\",\n",
    "            verbose=True,\n",
    "            tools=[self.compare_products]\n",
    "        )\n",
    "\n",
    "        # Recommendation Agent\n",
    "        self.recommendation_agent = Agent(\n",
    "            role='Product Recommender',\n",
    "            goal='Provide personalized product recommendations',\n",
    "            backstory=\"\"\"You use advanced ML models to provide personalized product recommendations \n",
    "            based on user preferences and history.\"\"\",\n",
    "            verbose=True,\n",
    "            tools=[self.get_recommendations]\n",
    "        )\n",
    "\n",
    "        # General Information Agent\n",
    "        self.info_agent = Agent(\n",
    "            role='Information Provider',\n",
    "            goal='Provide accurate product and general healthcare information',\n",
    "            backstory=\"\"\"You are knowledgeable about all products and general healthcare information. \n",
    "            You provide detailed and accurate information while maintaining compliance.\"\"\",\n",
    "            verbose=True,\n",
    "            tools=[self.get_product_info, self.get_healthcare_info]\n",
    "        )\n",
    "\n",
    "    def verify_safety(self, query: str) -> bool:\n",
    "        \"\"\"Verify if the query is safe and compliant\"\"\"\n",
    "        # Implement safety verification logic\n",
    "        return True\n",
    "\n",
    "    def process_order(self, order_details: Dict) -> Dict:\n",
    "        \"\"\"Process a new order\"\"\"\n",
    "        # Implement order processing logic\n",
    "        return {\"order_id\": \"123\", \"status\": \"processed\"}\n",
    "\n",
    "    def check_order_status(self, order_id: str) -> Dict:\n",
    "        \"\"\"Check status of an existing order\"\"\"\n",
    "        # Implement order status check logic\n",
    "        return {\"order_id\": order_id, \"status\": \"in progress\"}\n",
    "\n",
    "    def update_cart(self, action: str, product_id: str, quantity: int) -> Dict:\n",
    "        \"\"\"Update shopping cart\"\"\"\n",
    "        # Implement cart update logic\n",
    "        return {\"status\": \"success\", \"cart_items\": []}\n",
    "\n",
    "    def display_cart(self) -> List[Dict]:\n",
    "        \"\"\"Display current cart contents\"\"\"\n",
    "        # Implement cart display logic\n",
    "        return [{\"product_id\": \"123\", \"quantity\": 1, \"price\": 99.99}]\n",
    "\n",
    "    def compare_products(self, product_id: str) -> Dict:\n",
    "        \"\"\"Compare product with market alternatives\"\"\"\n",
    "        # Implement product comparison logic\n",
    "        return {\n",
    "            \"product\": \"Product A\",\n",
    "            \"competitors\": [\n",
    "                {\"name\": \"Product B\", \"price\": 89.99, \"features\": []}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def get_recommendations(self, user_id: str) -> List[Dict]:\n",
    "        \"\"\"Get personalized product recommendations\"\"\"\n",
    "        # Implement recommendation logic using the trained model\n",
    "        return [{\"product_id\": \"123\", \"score\": 0.95}]\n",
    "\n",
    "    def get_product_info(self, product_id: str) -> Dict:\n",
    "        \"\"\"Get detailed product information\"\"\"\n",
    "        # Implement product info retrieval logic\n",
    "        return {\"id\": product_id, \"name\": \"Product A\", \"details\": {}}\n",
    "\n",
    "    def get_healthcare_info(self, query: str) -> str:\n",
    "        \"\"\"Get general healthcare information\"\"\"\n",
    "        # Implement healthcare info retrieval logic\n",
    "        return \"Healthcare information response\"\n",
    "\n",
    "    def create_tasks(self, user_query: str) -> List[Task]:\n",
    "        # Safety Check Task\n",
    "        safety_task = Task(\n",
    "            description=f\"Verify safety and compliance of query: {user_query}\",\n",
    "            agent=self.security_agent\n",
    "        )\n",
    "\n",
    "        # Classification Task\n",
    "        classification_task = Task(\n",
    "            description=f\"Classify user query intent: {user_query}\",\n",
    "            agent=self.classifier_agent\n",
    "        )\n",
    "\n",
    "        # Dynamic task creation based on classification\n",
    "        # This would be implemented based on the classification result\n",
    "        return [safety_task, classification_task]\n",
    "\n",
    "    def process_query(self, user_query: str) -> Dict:\n",
    "        \"\"\"Main method to process user queries\"\"\"\n",
    "        # Create crew\n",
    "        crew = Crew(\n",
    "            agents=[\n",
    "                self.security_agent,\n",
    "                self.classifier_agent,\n",
    "                self.order_agent,\n",
    "                self.cart_agent,\n",
    "                self.comparison_agent,\n",
    "                self.recommendation_agent,\n",
    "                self.info_agent\n",
    "            ],\n",
    "            tasks=self.create_tasks(user_query),\n",
    "            process=Process.sequential\n",
    "        )\n",
    "\n",
    "        # Execute crew\n",
    "        result = crew.kickoff()\n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the crew\n",
    "    healthcare_crew = HealthcareEcommerceCrew(\n",
    "        openai_api_key=\"your-openai-key\",\n",
    "        pinecone_api_key=\"your-pinecone-key\",\n",
    "        pinecone_environment=\"your-environment\"\n",
    "    )\n",
    "    \n",
    "    # Create agents\n",
    "    healthcare_crew.create_agents()\n",
    "    \n",
    "    # Process a sample query\n",
    "    result = healthcare_crew.process_query(\n",
    "        \"Can you recommend supplements for joint health and show me what's in my cart?\"\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
