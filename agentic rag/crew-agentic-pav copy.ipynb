{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2f2621-074e-45a3-b892-9fd4ac5dbc2a",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# Agentic RAG Using CrewAI & LangChain\n",
    "\n",
    "We are going to see how agents can be involved in the RAG system to rtrieve the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2408f415-d51c-4d75-ad71-e809b7072778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T08:52:51.355586Z",
     "iopub.status.busy": "2024-08-06T08:52:51.355253Z",
     "iopub.status.idle": "2024-08-06T08:53:07.107101Z",
     "shell.execute_reply": "2024-08-06T08:53:07.105613Z",
     "shell.execute_reply.started": "2024-08-06T08:52:51.355560Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install crewai==0.28.8 crewai_tools==0.1.6 langchain_community==0.0.29 sentence-transformers langchain-groq --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d74a183-c473-4286-aa91-6ac1c89f8a08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T08:53:11.142412Z",
     "iopub.status.busy": "2024-08-06T08:53:11.141324Z",
     "iopub.status.idle": "2024-08-06T08:53:17.696025Z",
     "shell.execute_reply": "2024-08-06T08:53:17.695307Z",
     "shell.execute_reply.started": "2024-08-06T08:53:11.142373Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from crewai_tools import PDFSearchTool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from crewai_tools  import tool\n",
    "from crewai import Crew,LLM\n",
    "from crewai import Task\n",
    "from crewai import Agent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61808e55-3a5c-4977-a36f-6487740d20c9",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Mention the LLM being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb3e7b8-04c1-4a4b-a78f-9f10cef13222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T08:54:42.224934Z",
     "iopub.status.busy": "2024-08-06T08:54:42.224560Z",
     "iopub.status.idle": "2024-08-06T08:54:42.268360Z",
     "shell.execute_reply": "2024-08-06T08:54:42.267668Z",
     "shell.execute_reply.started": "2024-08-06T08:54:42.224905Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=\"groq/llama-3.3-70b-versatile\",\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be093f01-bb3d-483e-9ea2-0ca9c5875000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:09:42.443927Z",
     "iopub.status.busy": "2024-08-06T09:09:42.443522Z",
     "iopub.status.idle": "2024-08-06T09:09:43.219922Z",
     "shell.execute_reply": "2024-08-06T09:09:43.219238Z",
     "shell.execute_reply.started": "2024-08-06T09:09:42.443896Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "pdf_url = 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "with open('attenstion_is_all_you_need.pdf', 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cfdc74-f167-4a14-a4d6-6094cb5b71a0",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Create a RAG tool variable to pass our PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97d2f22-8667-4799-bf90-1a9fec1dc540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:10:08.404340Z",
     "iopub.status.busy": "2024-08-06T09:10:08.403918Z",
     "iopub.status.idle": "2024-08-06T09:10:12.883348Z",
     "shell.execute_reply": "2024-08-06T09:10:12.880732Z",
     "shell.execute_reply.started": "2024-08-06T09:10:08.404297Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\embedchain\\embedder\\huggingface.py:34: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=self.config.model, model_kwargs=self.config.model_kwargs)\n",
      "C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "rag_tool = PDFSearchTool(pdf='attenstion_is_all_you_need.pdf',\n",
    "    config=dict(\n",
    "        llm=dict(\n",
    "            provider=\"groq\", # or google, openai, anthropic, llama2, ...\n",
    "            config=dict(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                # temperature=0.5,\n",
    "                # top_p=1,\n",
    "                # stream=true,\n",
    "            ),\n",
    "        ),\n",
    "        embedder=dict(\n",
    "            provider=\"huggingface\", # or openai, ollama, ...\n",
    "            config=dict(\n",
    "                model=\"BAAI/bge-small-en-v1.5\",\n",
    "                #task_type=\"retrieval_document\",\n",
    "                # title=\"Embeddings\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e49ecd-7cd8-464c-be2f-5e4aa1f58de6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:10:29.400488Z",
     "iopub.status.busy": "2024-08-06T09:10:29.399575Z",
     "iopub.status.idle": "2024-08-06T09:10:29.479174Z",
     "shell.execute_reply": "2024-08-06T09:10:29.475340Z",
     "shell.execute_reply.started": "2024-08-06T09:10:29.400441Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Search a PDF's content\n",
      "Relevant Content:\n",
      "a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This\n",
      "\n",
      "[9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2\n",
      "\n",
      "Figure 1: The Transformer - model architecture. wise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm(x+ Sublayer(x), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined\n"
     ]
    }
   ],
   "source": [
    "res = rag_tool.run(\"what is transformer?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac98c3c-b02f-4680-8fab-99fbfa3c45a3",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Mention the Tavily API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "086f3f46-0750-4d59-a56a-02d13b802223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:10:40.163940Z",
     "iopub.status.busy": "2024-08-06T09:10:40.163524Z",
     "iopub.status.idle": "2024-08-06T09:10:40.167780Z",
     "shell.execute_reply": "2024-08-06T09:10:40.167028Z",
     "shell.execute_reply.started": "2024-08-06T09:10:40.163904Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 13:29:47,219 - 19756 - _common.py-_common:105 - INFO: Backing off send_request(...) for 0.6s (posthog.request.APIError: [PostHog] upstream connect error or disconnect/reset before headers. reset reason: overflow (503))\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b445a6f5-ff84-4ee8-b633-f28be3d06b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:10:41.783474Z",
     "iopub.status.busy": "2024-08-06T09:10:41.782945Z",
     "iopub.status.idle": "2024-08-06T09:10:43.410666Z",
     "shell.execute_reply": "2024-08-06T09:10:43.409742Z",
     "shell.execute_reply.started": "2024-08-06T09:10:41.783445Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://incubity.ambilio.com/self-attention-mechanism-in-transformer-based-llms/',\n",
       "  'content': 'Role of Self-Attention in Large Language Models Self-attention plays a crucial role in empowering Transformer-based large language models to achieve remarkable performance in NLP tasks. Self-Attention in BERT and GPT Models Both BERT and GPT, two of the most prominent Transformer-based language models, leverage self-attention in their architectures. During training, GPT predicts the next word in a sequence based on the preceding words, with self-attention capturing dependencies across the generated text. In conclusion, the self-attention mechanism serves as a foundational building block in Transformer-based large language models, enabling them to comprehend and generate natural language with unparalleled accuracy and fluency. By capturing long-range dependencies and facilitating efficient information processing, self-attention empowers these models to excel across a wide range of NLP tasks.'},\n",
       " {'url': 'https://medium.com/data-science-community-srm/exploring-attention-mechanisms-in-large-language-models-33549ae2c5b8',\n",
       "  'content': \"These models, think of them as super-smart text processors, use something called 'self-attention.' Now, self-attention is a feature in their design that helps them understand words and\"},\n",
       " {'url': 'https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.5-understanding-self-attention/',\n",
       "  'content': 'References. Attention Is All You Need (2018) — the original transformer paper; What we covered in this video lecture. To understand large language transformers, it is essential to understand self-attention, which is the underlying mechanism that powers these models: self-attention can be understood as a way to create context-aware text embedding vectors.'},\n",
       " {'url': 'https://python.plainenglish.io/attention-mechanism-in-a-large-language-model-unlocking-the-power-of-context-d797dc55f5e3',\n",
       "  'content': 'Large Language Models (LLMs), like GPT, have transformed the way machines understand and generate human language. But have you ever wondered what makes these models so intelligent? The secret lies in the attention mechanism , a game-changing innovation that allows models to focus on the most relevant pieces of information in a sequence.'},\n",
       " {'url': 'https://www.geeksforgeeks.org/self-attention-in-nlp/',\n",
       "  'content': 'Self-Attention Mechanism. Natural language processing (NLP) tasks have been revolutionised by transformer-based models, in which the self-attention mechanism is an essential component. ... The reason behind that is if the dot products become large, this causes some self-attention scores to be very small after we apply softmax function in the'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search_tool.run(\"What is self-attention mechansim in large language models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e69851-f7e3-4b9a-8c7c-7da0f7f04bde",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Define a Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "907b371c-4b24-4c80-b833-971f56ad4c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:11:34.946830Z",
     "iopub.status.busy": "2024-08-06T09:11:34.946268Z",
     "iopub.status.idle": "2024-08-06T09:11:34.952558Z",
     "shell.execute_reply": "2024-08-06T09:11:34.951976Z",
     "shell.execute_reply.started": "2024-08-06T09:11:34.946789Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def router_tool(question):\n",
    "  \"\"\"Router Function\"\"\"\n",
    "  if 'self-attention' in question:\n",
    "    return 'vectorstore'\n",
    "  else:\n",
    "    return 'web_search'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be17dc59-0e72-4eaa-9654-400df6cb371e",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Create Agents to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6cacf6a3-7f50-49d6-9ab0-b63433d7032c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:11:39.241558Z",
     "iopub.status.busy": "2024-08-06T09:11:39.241051Z",
     "iopub.status.idle": "2024-08-06T09:11:39.249728Z",
     "shell.execute_reply": "2024-08-06T09:11:39.248634Z",
     "shell.execute_reply.started": "2024-08-06T09:11:39.241517Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Router_Agent = Agent(\n",
    "  role='Router',\n",
    "  goal='Route user question to a vectorstore or web search',\n",
    "  backstory=(\n",
    "    \"You are an expert at routing a user question to a vectorstore or web search.\"\n",
    "    \"Use the vectorstore for questions on concept related to Retrieval-Augmented Generation.\"\n",
    "    \"You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search.\"\n",
    "  ),\n",
    "  verbose=True,\n",
    "  allow_delegation=False,\n",
    "  llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fc04747-af23-4b19-9ece-6473c683f739",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:11:41.155843Z",
     "iopub.status.busy": "2024-08-06T09:11:41.155481Z",
     "iopub.status.idle": "2024-08-06T09:11:41.162015Z",
     "shell.execute_reply": "2024-08-06T09:11:41.161341Z",
     "shell.execute_reply.started": "2024-08-06T09:11:41.155815Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Retriever_Agent = Agent(\n",
    "role=\"Retriever\",\n",
    "goal=\"Use the information retrieved from the vectorstore to answer the question\",\n",
    "backstory=(\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the information present in the retrieved context to answer the question.\"\n",
    "    \"You have to provide a clear concise answer.\"\n",
    "),\n",
    "verbose=True,\n",
    "allow_delegation=False,\n",
    "llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25ece7eb-37fa-450d-991b-16dd8abca495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:11:42.476081Z",
     "iopub.status.busy": "2024-08-06T09:11:42.475654Z",
     "iopub.status.idle": "2024-08-06T09:11:42.489959Z",
     "shell.execute_reply": "2024-08-06T09:11:42.485582Z",
     "shell.execute_reply.started": "2024-08-06T09:11:42.476042Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Grader_agent =  Agent(\n",
    "  role='Answer Grader',\n",
    "  goal='Filter out erroneous retrievals',\n",
    "  backstory=(\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question.\"\n",
    "    \"If the document contains keywords related to the user question, grade it as relevant.\"\n",
    "    \"It does not need to be a stringent test.You have to make sure that the answer is relevant to the question.\"\n",
    "  ),\n",
    "  verbose=True,\n",
    "  allow_delegation=False,\n",
    "  llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5cdb03f9-34bc-400b-8ada-80c08a71f1f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:11:43.725087Z",
     "iopub.status.busy": "2024-08-06T09:11:43.724709Z",
     "iopub.status.idle": "2024-08-06T09:11:43.731283Z",
     "shell.execute_reply": "2024-08-06T09:11:43.730399Z",
     "shell.execute_reply.started": "2024-08-06T09:11:43.725058Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hallucination_grader = Agent(\n",
    "    role=\"Hallucination Grader\",\n",
    "    goal=\"Filter out hallucination\",\n",
    "    backstory=(\n",
    "        \"You are a hallucination grader assessing whether an answer is grounded in / supported by a set of facts.\"\n",
    "        \"Make sure you meticulously review the answer and check if the response provided is in alignmnet with the question asked\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6e3e019-18c4-4f33-be69-628274a544e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:11:48.359638Z",
     "iopub.status.busy": "2024-08-06T09:11:48.359179Z",
     "iopub.status.idle": "2024-08-06T09:11:48.372283Z",
     "shell.execute_reply": "2024-08-06T09:11:48.369019Z",
     "shell.execute_reply.started": "2024-08-06T09:11:48.359547Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "answer_grader = Agent(\n",
    "    role=\"Answer Grader\",\n",
    "    goal=\"Filter out hallucination from the answer.\",\n",
    "    backstory=(\n",
    "        \"You are a grader assessing whether an answer is useful to resolve a question.\"\n",
    "        \"Make sure you meticulously review the answer and check if it makes sense for the question asked\"\n",
    "        \"If the answer is relevant generate a clear and concise response.\"\n",
    "        \"If the answer gnerated is not relevant then perform a websearch using 'web_search_tool'\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626670c9-717e-4bcd-b0b4-e4d9aaaae7eb",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Defining Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e9c9242-5b00-4e59-8e3d-d904c8e9d21f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:11:50.881204Z",
     "iopub.status.busy": "2024-08-06T09:11:50.880708Z",
     "iopub.status.idle": "2024-08-06T09:11:50.886480Z",
     "shell.execute_reply": "2024-08-06T09:11:50.885321Z",
     "shell.execute_reply.started": "2024-08-06T09:11:50.881168Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "router_task = Task(\n",
    "    description=(\"Analyse the keywords in the question {question}\"\n",
    "    \"Based on the keywords decide whether it is eligible for a vectorstore search or a web search.\"\n",
    "    \"Return a single word 'vectorstore' if it is eligible for vectorstore search.\"\n",
    "    \"Return a single word 'websearch' if it is eligible for web search.\"\n",
    "    \"Do not provide any other premable or explaination.\"\n",
    "    ),\n",
    "    expected_output=(\"Give a binary choice 'websearch' or 'vectorstore' based on the question\"\n",
    "    \"Do not provide any other premable or explaination.\"),\n",
    "    agent=Router_Agent,\n",
    "    tools=[router_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a497afe-0fd0-4b9f-a97b-64d1b5518ef0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:11:52.682162Z",
     "iopub.status.busy": "2024-08-06T09:11:52.681767Z",
     "iopub.status.idle": "2024-08-06T09:11:52.689821Z",
     "shell.execute_reply": "2024-08-06T09:11:52.689037Z",
     "shell.execute_reply.started": "2024-08-06T09:11:52.682135Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retriever_task = Task(\n",
    "    description=(\"Based on the response from the router task extract information for the question {question} with the help of the respective tool.\"\n",
    "    \"Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.\"\n",
    "    \"Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\"\n",
    "    ),\n",
    "    expected_output=(\"You should analyse the output of the 'router_task'\"\n",
    "    \"If the response is 'websearch' then use the web_search_tool to retrieve information from the web.\"\n",
    "    \"If the response is 'vectorstore' then use the rag_tool to retrieve information from the vectorstore.\"\n",
    "    \"Return a claer and consise text as response.\"),\n",
    "    agent=Retriever_Agent,\n",
    "    context=[router_task],\n",
    "   #tools=[retriever_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce51750e-941d-41fa-b4d4-d36f7e56961e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:11:54.826584Z",
     "iopub.status.busy": "2024-08-06T09:11:54.825901Z",
     "iopub.status.idle": "2024-08-06T09:11:54.831307Z",
     "shell.execute_reply": "2024-08-06T09:11:54.830655Z",
     "shell.execute_reply.started": "2024-08-06T09:11:54.826545Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "grader_task = Task(\n",
    "    description=(\"Based on the response from the retriever task for the quetion {question} evaluate whether the retrieved content is relevant to the question.\"\n",
    "    ),\n",
    "    expected_output=(\"Binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\"\n",
    "    \"You must answer 'yes' if the response from the 'retriever_task' is in alignment with the question asked.\"\n",
    "    \"You must answer 'no' if the response from the 'retriever_task' is not in alignment with the question asked.\"\n",
    "    \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"),\n",
    "    agent=Grader_agent,\n",
    "    context=[retriever_task],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d1f89f1-caef-43ea-9ec7-614467f73a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:11:58.436290Z",
     "iopub.status.busy": "2024-08-06T09:11:58.435873Z",
     "iopub.status.idle": "2024-08-06T09:11:58.443357Z",
     "shell.execute_reply": "2024-08-06T09:11:58.442433Z",
     "shell.execute_reply.started": "2024-08-06T09:11:58.436248Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hallucination_task = Task(\n",
    "    description=(\"Based on the response from the grader task for the quetion {question} evaluate whether the answer is grounded in / supported by a set of facts.\"),\n",
    "    expected_output=(\"Binary score 'yes' or 'no' score to indicate whether the answer is sync with the question asked\"\n",
    "    \"Respond 'yes' if the answer is in useful and contains fact about the question asked.\"\n",
    "    \"Respond 'no' if the answer is not useful and does not contains fact about the question asked.\"\n",
    "    \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"),\n",
    "    agent=hallucination_grader,\n",
    "    context=[grader_task],\n",
    ")\n",
    "\n",
    "answer_task = Task(\n",
    "    description=(\"Based on the response from the hallucination task for the quetion {question} evaluate whether the answer is useful to resolve the question.\"\n",
    "    \"If the answer is 'yes' return a clear and concise answer.\"\n",
    "    \"If the answer is 'no' then perform a 'websearch' and return the response\"),\n",
    "    expected_output=(\"Return a clear and concise response if the response from 'hallucination_task' is 'yes'.\"\n",
    "    \"Perform a web search using 'web_search_tool' and return ta clear and concise response only if the response from 'hallucination_task' is 'no'.\"\n",
    "    \"Otherwise respond as 'Sorry! unable to find a valid response'.\"),\n",
    "    context=[hallucination_task],\n",
    "    agent=answer_grader,\n",
    "    #tools=[answer_grader_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190bfcbf-66bd-4ddc-bde6-2a96a002b8d7",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Define a flow for the use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75f9cbeb-d342-4da9-b442-b13e66ebbc96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:12:01.597715Z",
     "iopub.status.busy": "2024-08-06T09:12:01.597364Z",
     "iopub.status.idle": "2024-08-06T09:12:01.609224Z",
     "shell.execute_reply": "2024-08-06T09:12:01.608693Z",
     "shell.execute_reply.started": "2024-08-06T09:12:01.597687Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 11:35:01,186 - 3196 - __init__.py-__init__:537 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "rag_crew = Crew(\n",
    "    agents=[Router_Agent, Retriever_Agent, Grader_agent, hallucination_grader, answer_grader],\n",
    "    tasks=[router_task, retriever_task, grader_task, hallucination_task, answer_task],\n",
    "    verbose=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957af34a-ffc4-4283-9e26-3c936f8719cc",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Ask any query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b98e365a-cd84-4664-941e-5fb40765d7dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:12:12.076469Z",
     "iopub.status.busy": "2024-08-06T09:12:12.075913Z",
     "iopub.status.idle": "2024-08-06T09:12:12.089371Z",
     "shell.execute_reply": "2024-08-06T09:12:12.082684Z",
     "shell.execute_reply.started": "2024-08-06T09:12:12.076435Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs ={\"question\":\"How does self-attention mechanism help large language models?\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebcad1-ab52-4ed9-a065-485cac2a4362",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Kick off the agent pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fcd4ac6-a176-425f-8bf9-a15ef0075381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:12:16.098251Z",
     "iopub.status.busy": "2024-08-06T09:12:16.097908Z",
     "iopub.status.idle": "2024-08-06T09:12:22.448360Z",
     "shell.execute_reply": "2024-08-06T09:12:22.447696Z",
     "shell.execute_reply.started": "2024-08-06T09:12:16.098211Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRouter\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyse the keywords in the question How does self-attention mechanism help large language models?Based on the keywords decide whether it is eligible for a vectorstore search or a web search.Return a single word 'vectorstore' if it is eligible for vectorstore search.Return a single word 'websearch' if it is eligible for web search.Do not provide any other premable or explaination.\u001b[00m\n",
      "\u001b[91m \n",
      "\n",
      "I encountered an error while trying to use the tool. This was the error: router_tool() missing 1 required positional argument: 'question'.\n",
      " Tool router_tool accepts these inputs: Tool Name: router_tool\n",
      "Tool Arguments: {}\n",
      "Tool Description: Router Function\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRouter\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: The question contains keywords related to large language models and self-attention mechanisms, which are concepts related to Retrieval-Augmented Generation. I should use the vectorstore search for this question.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mrouter_tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"question\\\": \\\"How does self-attention mechanism help large language models?\\\", \\\"keywords\\\": [\\\"self-attention\\\", \\\"large language models\\\", \\\"retrieval-augmented generation\\\"]}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "\n",
      "I encountered an error while trying to use the tool. This was the error: router_tool() missing 1 required positional argument: 'question'.\n",
      " Tool router_tool accepts these inputs: Tool Name: router_tool\n",
      "Tool Arguments: {}\n",
      "Tool Description: Router Function.\n",
      "Moving on then. I MUST either use a tool (use one at time) OR give my best final answer not both at the same time. To Use the following format:\n",
      "\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [router_tool]\n",
      "Action Input: the input to the action, dictionary enclosed in curly braces\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Result can repeat N times)\n",
      "Thought: I now can give a great answer\n",
      "Final Answer: Your final answer must be the great and the most complete as possible, it must be outcome described\n",
      "\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRouter\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "vectorstore\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRetriever\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the router task extract information for the question How does self-attention mechanism help large language models? with the help of the respective tool.Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRetriever\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "The self-attention mechanism is a key component of large language models, such as transformers, and allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. It works by computing attention weights based on the input sequence, which are then used to compute a weighted sum of the input elements. The benefits of the self-attention mechanism include parallelization, improved performance, and better handling of long-range dependencies. The self-attention mechanism has been shown to improve the performance of large language models on a range of natural language processing tasks, including language translation, question answering, and text classification.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the retriever task for the quetion How does self-attention mechanism help large language models? evaluate whether the retrieved content is relevant to the question.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "yes\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mHallucination Grader\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the grader task for the quetion How does self-attention mechanism help large language models? evaluate whether the answer is grounded in / supported by a set of facts.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mHallucination Grader\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "yes\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the hallucination task for the quetion How does self-attention mechanism help large language models? evaluate whether the answer is useful to resolve the question.If the answer is 'yes' return a clear and concise answer.If the answer is 'no' then perform a 'websearch' and return the response\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "The self-attention mechanism is a key component in large language models, particularly in transformer-based architectures. It allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is particularly useful for natural language processing tasks, as it enables the model to capture long-range dependencies and contextual relationships between words.\n",
      "\n",
      "In essence, self-attention mechanisms help large language models in several ways:\n",
      "\n",
      "1. **Parallelization**: Self-attention allows for parallelization of sequential computations, making it much faster than traditional recurrent neural networks (RNNs) for long sequences.\n",
      "2. **Contextual understanding**: By attending to different parts of the input sequence, the model can capture a broader context and understand the relationships between words, phrases, and ideas.\n",
      "3. **Improved handling of long-range dependencies**: Self-attention helps the model to handle long-range dependencies, which are common in natural language, by allowing it to attend to distant parts of the input sequence.\n",
      "4. **Flexibility and generalizability**: The self-attention mechanism can be applied to various natural language processing tasks, such as language translation, question answering, and text summarization.\n",
      "\n",
      "Overall, the self-attention mechanism is a crucial component of large language models, enabling them to efficiently process and understand complex natural language inputs.\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = rag_crew.kickoff(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf9e58d-3c00-4cea-83d9-806a016f740b",
   "metadata": {
    "language": "python"
   },
   "source": [
    "## Obtain the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49e437fa-c3b3-4d1e-983b-93713a2df989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T09:12:44.362032Z",
     "iopub.status.busy": "2024-08-06T09:12:44.361665Z",
     "iopub.status.idle": "2024-08-06T09:12:44.366198Z",
     "shell.execute_reply": "2024-08-06T09:12:44.365344Z",
     "shell.execute_reply.started": "2024-08-06T09:12:44.362005Z"
    },
    "language": "python",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The self-attention mechanism is a key component in large language models, particularly in transformer-based architectures. It allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is particularly useful for natural language processing tasks, as it enables the model to capture long-range dependencies and contextual relationships between words.\n",
      "\n",
      "In essence, self-attention mechanisms help large language models in several ways:\n",
      "\n",
      "1. **Parallelization**: Self-attention allows for parallelization of sequential computations, making it much faster than traditional recurrent neural networks (RNNs) for long sequences.\n",
      "2. **Contextual understanding**: By attending to different parts of the input sequence, the model can capture a broader context and understand the relationships between words, phrases, and ideas.\n",
      "3. **Improved handling of long-range dependencies**: Self-attention helps the model to handle long-range dependencies, which are common in natural language, by allowing it to attend to distant parts of the input sequence.\n",
      "4. **Flexibility and generalizability**: The self-attention mechanism can be applied to various natural language processing tasks, such as language translation, question answering, and text summarization.\n",
      "\n",
      "Overall, the self-attention mechanism is a crucial component of large language models, enabling them to efficiently process and understand complex natural language inputs.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8da899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e878ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0db2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupyterlab": {
   "notebooks": {
    "version_major": 6,
    "version_minor": 4
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "singlestore_cell_default_language": "python",
  "singlestore_connection": {
   "connectionID": "a22b6f8b-b11b-4979-98da-98513e9876e6",
   "defaultDatabase": ""
  },
  "singlestore_row_limit": 300
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
